{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import string\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics # confusion matrix, MSE etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = \"/Users/t.z.cheng/Google_Drive/Research/Delaydoesmatter/real_exp/exp4_20CR12/4c/results/session-62159fb8dfe54372fbc126e5-data.csv\"\n",
    "path_to_data = \"/Users/t.z.cheng/Google_Drive/Research/cross_domain_entrainment/exp8/results/session-635300ef7f3ee915ba688e53-data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read the data\n",
    "df = pd.read_csv(path_to_data)\n",
    "# df.groupby('stimuli_presented').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tasks & Conditions: Clean up task and subject ID (first five characters)\n",
    "df['task'] = df['trial_template'].apply(lambda x: x.split(\"_\")[0])\n",
    "df['sub_id'] = df['participant_id'].apply(lambda x: x.split()[0][0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add the accuracy and PPS column to the dataset: transform True and Shorter to 1, False and longer to 0\n",
    "Correct = [] # only applied for the prescreen\n",
    "Shorter = []\n",
    "for i in np.arange(0,len(df)):\n",
    "    if df['response_correct'][i] == True:\n",
    "        Correct.append(1)\n",
    "    else: \n",
    "        Correct.append(0)\n",
    "    if df['response_value'][i] == \"Shorter\":\n",
    "        Shorter.append(1)\n",
    "    else: \n",
    "        Shorter.append(0)\n",
    "df['Correct'] = Correct\n",
    "df['Shorter'] = Shorter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Total participated subjects\n",
    "len(df['sub_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for exp8abc\n",
    "df['exp'] = df['group_id'].apply(lambda x: x.split(\"_\")[0])\n",
    "df_exp8c = df[(df['exp'] == 'EXP8c')].reset_index(drop = True)\n",
    "df = df_exp8c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['exp'] = df['group_id'].apply(lambda x: x.split(\"_\")[0])\n",
    "len(df.groupby('exp')['sub_id'].unique()['EXP8a'])\n",
    "len(df.groupby('exp')['sub_id'].unique()['EXP8b'])\n",
    "len(df.groupby('exp')['sub_id'].unique()['EXP8c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.groupby('exp')['sub_id'].unique()['EXP8a'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set parameters\n",
    "- Accuracy threshold for the easiest trials of the main task \n",
    "- Extreme RT threshold\n",
    "- Catch trial accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parameters\n",
    "threshold = .55\n",
    "catch_threshold = .7\n",
    "RT_threshold = 10000\n",
    "practice_threshold = 'pass'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning \n",
    "***Super important: df_clean is overwritten after each step of data cleaning***\n",
    "\n",
    "**Reject subjects**\n",
    "- Catch trial\n",
    "- Environmental noise & audio device (may not need to use them as criteria)\n",
    "- Fail the practice trial on the last 12 practice trial: pass people have done 24 trials, fail people have done 36 trials\n",
    "\n",
    "**Reject trials**\n",
    "- Task-relevant trials\n",
    "- Extreme RT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## How many subjects miss the catch trial \n",
    "catch_trials = ['Catch_cat','Catch_bird']\n",
    "catch_trials = ['Catch_trials'] ## for exp8abc\n",
    "\n",
    "df_catch = df[(df['trial_template'].isin(catch_trials))].reset_index(drop = True) # reset index from 1\n",
    "catch_acc = df_catch.groupby('sub_id')['response_correct'].sum()/df_catch.groupby('sub_id')['response_correct'].count()\n",
    "fail_catch = catch_acc[catch_acc < catch_threshold].reset_index()\n",
    "fail_catch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## remove subjects who failed the catch trial based on the threshold\n",
    "df_clean = df[~df['sub_id'].isin(fail_catch['sub_id'])].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## How many subjects had a bad environmental noise and device\n",
    "df_noise = df[(df['response_name'] == 'survey_noise')].reset_index(drop = True)\n",
    "noise = df_noise.groupby('sub_id')['response_value'].sum()\n",
    "too_noisy = noise[noise == '4'].reset_index()\n",
    "too_noisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## remove subjects who completed the task in noisy environment\n",
    "df_clean = df_clean[~df_clean['sub_id'].isin(too_noisy['sub_id'])].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# may remove subjects who completed the task using low-quality audio device\n",
    "df_device = df[(df['response_name'] == 'survey_headphone1')].reset_index()\n",
    "len(df_device[['sub_id','response_value']] == 'Yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## remove subjects who failed the practice \n",
    "fail_prac = df[df['branch_failpass'] == 'fail']['sub_id'].unique()\n",
    "df_clean = df_clean[(~df_clean['sub_id'].isin(fail_prac))].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## After cleaning how many subjects left\n",
    "len(df_clean['sub_id'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task relevant trials\n",
    "#### Select the trials to analyze\n",
    "- Pretest trails (n = 60): single tone 1 or 8\n",
    "- Practice trials (n = 24 or 36): 6 context tones + 1 ontime single tone 1 or 8 with correct/incorrect feedback\n",
    "- Main task (n = 576): 6 context tones + 1 single tone (early, ontime, late; 1 to 8 steps) without feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the pretest trials "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre = df_clean[(df_clean['task'] == \"PretestTrials\")].reset_index()\n",
    "df_pre.groupby(['sub_id','stimuli_presented'])['Correct'].mean()\n",
    "pre_acc = df_pre.groupby(['sub_id'])['Correct'].mean()\n",
    "conds = df_pre['stimuli_presented'].unique()\n",
    "print(conds) # check the conditions\n",
    "df_pre.groupby('stimuli_presented')['Shorter'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the practice trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prac = df_clean[(df_clean['task'] == \"practiceTrials\")].reset_index()\n",
    "conds = df_prac['stimuli_presented'].unique()\n",
    "print(conds) # check the conditions\n",
    "df_prac.groupby('stimuli_presented')['Shorter'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_all = df_clean # create a new variable just in case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ??? under progress, need to write a code to save those *fail* who but pass on the last 12 trials??? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pra = df_pra[-12:-1]\n",
    "# df_pra.groupby(['sub_id','stimuli_presented'])['Correct'].mean()\n",
    "# pra_acc = df_pra.groupby(['sub_id'])['Correct'].mean()\n",
    "# fail_pra = pra_acc[pra_acc != practice_threshold].reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the main trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_clean[(df_clean['task'] == \"maintaskTrials\")].reset_index(drop = True)\n",
    "conds = df_clean['stimuli_presented'].unique()\n",
    "print(conds) # check the conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create new cols for onset and length\n",
    "onset = []\n",
    "length = []\n",
    "df_clean = df_clean.set_index(pd.Index(np.arange(0,len(df_clean)))) # change the index for the for loop\n",
    "\n",
    "for i in np.arange(0,len(df_clean)):\n",
    "    stimuli_presented = df_clean['stimuli_presented'][i].split('_')\n",
    "    onset.append(stimuli_presented[2])\n",
    "    length.append(stimuli_presented[-1])\n",
    "df_clean['Onset'] = onset\n",
    "df_clean['Length'] = length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Number of subjects, trials & conditions for each subjects\n",
    "n_subj = len(df_clean['participant_id'].unique())\n",
    "n_trial = len(df_clean)//n_subj # how many trials per subject\n",
    "n_conds = len(df_clean['stimuli_presented'].unique())\n",
    "print('Participant_number:', n_subj,'Trial number:', n_trial,'Condition number:', n_conds, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.groupby(['Onset'])['Correct'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_all = df_clean.groupby(['stimuli_presented','Onset'])['Correct'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## consider outliers based on the accuracy of the easiest trials \n",
    "outliers = Acc_easiest < threshold\n",
    "outliers = outliers[outliers == True]\n",
    "df_clean_nooutlier = df_clean[~df_clean['participant_id'].isin(outliers.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualization: bar plot\n",
    "ACC_plot = pd.DataFrame({'Conditions': conds, 'accuracy': accuracy_all})\n",
    "ax = ACC_plot.plot.bar(rot = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proportion short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.groupby(['Onset'])['Shorter'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pps = df_clean.groupby(['Onset'])['Shorter'].mean()\n",
    "print('Proportion short')\n",
    "print('Early',pps[0]) \n",
    "print('Ontime',pps[2])\n",
    "print('Late',pps[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pps_all = df_clean.groupby(['stimuli_presented','Onset'])['Shorter'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualization: bar plot\n",
    "PPS_plot = pd.DataFrame({'Conditions': conds, 'PSS': pps_all})\n",
    "ax = PPS_plot.plot.bar(rot = 0)\n",
    "pps_all.plot(style='k--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pps_all[8:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pps_all[0:8].plot(style = 'o-') # early\n",
    "pps_all[8:16].plot(style ='ko-') # late\n",
    "ax = pps_all[16:24].plot(style ='ro-') # ontime\n",
    "ax.set_xticklabels([0,1,2,3,4,5,6,7,8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistic tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PPS = df_clean.groupby(['sub_id','Onset'])['Shorter'].mean()\n",
    "PPS_early = PPS[::3]\n",
    "PPS_ontime = PPS[1::3]\n",
    "PPS_late = PPS[2::3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stats.ttest_rel(PPS_early, PPS_late))\n",
    "print(stats.ttest_rel(PPS_early, PPS_ontime))\n",
    "print(stats.ttest_rel(PPS_late, PPS_ontime))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the onset and comparison length to the csv file for R analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save df to csv\n",
    "df_clean.to_csv(r'/Users/t.z.cheng/Google_Drive/Research/Delaydoesmatter/real_exp/exp4_20CR12/4c/results/EXP4c_clean_n59.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "e774977668b7c0ae8309835a5187aa7fbf7669e7d0bb59755bc63e573643edcd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
